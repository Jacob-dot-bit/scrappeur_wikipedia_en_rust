# Scrappeur Wikipedia en Rust ğŸ¦€

Un scrappeur web performant et interactif dÃ©veloppÃ© en Rust pour extraire automatiquement des informations depuis les pages Wikipedia.

## ğŸ“‹ Table des matiÃ¨res

- [Description](#description)
- [FonctionnalitÃ©s](#fonctionnalitÃ©s)
- [Architecture technique](#architecture-technique)
- [Structure du projet](#structure-du-projet)
- [DÃ©pendances](#dÃ©pendances)
- [Installation](#installation)
- [Utilisation](#utilisation)
- [Exemples](#exemples)
- [RÃ©solution des problÃ¨mes](#rÃ©solution-des-problÃ¨mes)

## ğŸ¯ Description

Ce projet permet de scraper des pages Wikipedia de deux faÃ§ons :
1. **URLs directes** : Fournir une liste d'URLs Wikipedia Ã  scraper
2. **Recherche par mot-clÃ©** : Rechercher un sujet et scraper automatiquement les rÃ©sultats

Les donnÃ©es extraites incluent :
- Le titre de la page
- Le rÃ©sumÃ© (premier paragraphe)
- Les sections et sous-sections
- Les liens internes vers d'autres pages
- Les images prÃ©sentes sur la page

Toutes les donnÃ©es sont organisÃ©es dans des **dossiers par recherche** avec un rÃ©sumÃ© global et sauvegardÃ©es en plusieurs formats (JSON, Markdown, TXT).

## âœ¨ FonctionnalitÃ©s

- âœ… **Recherche par mot-clÃ©** : Recherche automatique via l'API OpenSearch de Wikipedia
- âœ… **Nombre de rÃ©sultats personnalisable** : Choisir combien d'articles scraper (1-20)
- âœ… **Organisation par recherche** : Un dossier timestampÃ© par recherche avec tous les articles
- âœ… **RÃ©sumÃ© global** : Fichier `RESUME_RECHERCHE.md` avec statistiques et liens
- âœ… **URLs directes** : Scraping d'URLs spÃ©cifiques
- âœ… **Mode interactif** : Interface CLI guidÃ©e avec choix du nombre de rÃ©sultats
- âœ… **Arguments CLI** : Utilisation via ligne de commande avec clap
- âœ… **Multi-formats** : JSON, Markdown, TXT pour les diffÃ©rentes donnÃ©es
- âœ… **Filtrage intelligent** : Exclusion automatique des icÃ´nes et petites images
- âœ… **Support HTTPS** : Connexion sÃ©curisÃ©e avec rustls
- âœ… **RequÃªtes HTTP manuelles** : Construction manuelle des requÃªtes HTTP/HTTPS
- âœ… **Gestion des erreurs robuste**
- âœ… **Pause entre requÃªtes** : Respectueux des serveurs Wikipedia

## ğŸ—ï¸ Architecture technique

### Respect de la contrainte "Pas de lib pour les requÃªtes rÃ©seau"

**Notre approche :**

âœ… **RequÃªtes HTTP construites manuellement**
- Utilisation directe de `std::net::TcpStream` (bibliothÃ¨que standard)
- Construction manuelle des headers HTTP (`GET`, `Host`, `User-Agent`, etc.)
- Parsing manuel des rÃ©ponses HTTP (sÃ©paration headers/body)
- **Aucune** bibliothÃ¨que HTTP de haut niveau (`reqwest`, `hyper`, `curl`, etc.)

âš ï¸ **Exception nÃ©cessaire : TLS/HTTPS**
- Wikipedia **force HTTPS** (redirection automatique HTTP â†’ HTTPS)
- ImplÃ©mentation TLS manuelle = plusieurs mois de travail + risques sÃ©curitÃ©
- Solution : `rustls` = bibliothÃ¨que de **cryptographie**, pas de requÃªtes HTTP
- `rustls` fait uniquement le chiffrement TLS, nous gÃ©rons toujours HTTP manuellement

### Flux de donnÃ©es

```
Mot-clÃ© utilisateur
    â†“
API OpenSearch Wikipedia (JSON)
    â†“
Liste d'URLs
    â†“
RequÃªte HTTPS manuelle (TcpStream + rustls)
    â†“
HTML parsing (scraper)
    â†“
Extraction donnÃ©es
    â†“
Organisation en dossiers
    â†“
Sauvegarde fichiers (JSON + Markdown + TXT)
    â†“
GÃ©nÃ©ration RESUME_RECHERCHE.md
```

## ğŸ“ Structure du projet

```
Scrappeur wikipedia/
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main.rs              # Fichier principal (~700 lignes)
â”‚       â”œâ”€â”€ Structures
â”‚       â”‚   â”œâ”€â”€ WikipediaPage      # DonnÃ©es extraites
â”‚       â”‚   â””â”€â”€ Args               # Arguments CLI
â”‚       â”œâ”€â”€ Fonctions principales
â”‚       â”‚   â”œâ”€â”€ main()
â”‚       â”‚   â”œâ”€â”€ rechercher_wikipedia()
â”‚       â”‚   â”œâ”€â”€ scrape_wikipedia()
â”‚       â”‚   â””â”€â”€ generate_search_summary()
â”‚       â”œâ”€â”€ RÃ©seau HTTP/HTTPS
â”‚       â”‚   â”œâ”€â”€ http_get()
â”‚       â”‚   â”œâ”€â”€ https_get()
â”‚       â”‚   â”œâ”€â”€ extract_header()
â”‚       â”‚   â””â”€â”€ parse_url()
â”‚       â”œâ”€â”€ Extraction de contenu
â”‚       â”‚   â””â”€â”€ extract_summary()
â”‚       â”œâ”€â”€ Utilitaires
â”‚       â”‚   â”œâ”€â”€ url_encode()
â”‚       â”‚   â””â”€â”€ get_urls_interactif()
â”‚       â””â”€â”€ Sauvegarde
â”‚           â”œâ”€â”€ save_page_data()
â”‚           â””â”€â”€ generate_markdown()
â”‚
â”œâ”€â”€ resultats/               # Dossier gÃ©nÃ©rÃ© aprÃ¨s exÃ©cution
â”‚   â”œâ”€â”€ Avion_20240116_143025/     # Dossier de recherche
â”‚   â”‚   â”œâ”€â”€ RESUME_RECHERCHE.md   # â† RÃ©sumÃ© global de la recherche                
â”‚   â”‚   â”œâ”€â”€ avion.md    # Article 1
â”‚   â”‚   â”œâ”€â”€ Avion_de_ligne.md       # Article 2
â”‚   â”‚   â””â”€â”€ Boeing_747.md          # Article 3
â”‚   â”‚       â””â”€â”€ ...
â”‚   â””â”€â”€ BMW_20240116_150530/      # Autre recherche
â”‚       â”œâ”€â”€ RESUME_RECHERCHE.md
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ Cargo.toml              # Manifeste et dÃ©pendances
â”œâ”€â”€ Cargo.lock              # Verrouillage des versions
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸ“¦ DÃ©pendances

### DÃ©pendances principales

| Crate | Version | RÃ´le |
|-------|---------|------|
| **scraper** | 0.18 | Parser HTML avec sÃ©lecteurs CSS |
| **serde** | 1.0 | SÃ©rialisation des structures |
| **serde_json** | 1.0 | Export JSON |
| **clap** | 4.5 | Parser d'arguments CLI |
| **rustls** | 0.22 | ImplÃ©mentation TLS pure Rust |
| **webpki-roots** | 0.26 | Certificats racines pour TLS |
| **chrono** | 0.4 | Gestion des dates (timestamps) |
| **sanitize-filename** | 0.5 | Nettoyage des noms de fichiers |

### Pourquoi rustls ?

- âœ… **ImplÃ©mentation TLS pure Rust** (pas de dÃ©pendance OpenSSL)
- âœ… **Respecte la contrainte** : rustls est une lib de sÃ©curitÃ©/cryptographie, pas de requÃªtes HTTP
- âœ… **Nous construisons toujours les requÃªtes HTTP manuellement**
- âœ… **NÃ©cessaire** : Wikipedia force HTTPS

## ğŸš€ Installation

### PrÃ©requis

- **Rust** : Version 1.70 ou supÃ©rieure
- **Cargo** : Gestionnaire de paquets Rust (inclus avec Rust)

### Installer Rust

**Windows** :
```powershell
winget install Rustlang.Rustup
```

**Linux/macOS** :
```bash
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
```

### Cloner le projet

```bash
git clone https://github.com/Jacob-dot-bit/scrappeur_wikipedia_en_rust/
cd scrappeur_wikipedia_en_rust
```

### Compiler le projet

```bash
# Mode debug (dÃ©veloppement)
cargo build

# Mode release (optimisÃ©)
cargo build --release
```

## ğŸ’» Utilisation

### ğŸ” Mode 1 : Recherche par mot-clÃ©

**Ligne de commande** :
```bash
# Rechercher "avion" et scraper les 5 premiers rÃ©sultats (par dÃ©faut)
cargo run -- -k "avion"

# Rechercher et personnaliser le nombre de rÃ©sultats
cargo run -- -k "intelligence artificielle" -n 10

# Personnaliser le dossier de sortie
cargo run -- -k "Python" -n 5 --output mes_resultats
```

### ğŸ”— Mode 2 : URLs directes

**Avec un fichier** :
```bash
# CrÃ©er urls.txt avec vos URLs (une par ligne)
cargo run -- -f urls.txt
```

**Exemple de `urls.txt`** :
```
https://fr.wikipedia.org/wiki/Rust_(langage)
https://fr.wikipedia.org/wiki/Python_(langage)
https://fr.wikipedia.org/wiki/JavaScript
```

**En ligne de commande** :
```bash
# URLs sÃ©parÃ©es par des virgules
cargo run -- -u "https://fr.wikipedia.org/wiki/Rust_(langage),https://fr.wikipedia.org/wiki/Python_(langage)"
```

### ğŸ® Mode 3 : Interactif

**Sans arguments** :
```bash
cargo run
```

Menu interactif :
```
=== Scraper Wikipedia (Mode interactif) ===

Choisissez une option :
1. Entrer des URLs directement
2. Rechercher par mot-clÃ©

Votre choix (1-2) : 2
Entrez le mot-clÃ© Ã  rechercher : Avion
Nombre de rÃ©sultats Ã  scraper (dÃ©faut: 5, max 20) : 8
```

### ğŸ“– Aide complÃ¨te

```bash
cargo run -- --help
```

## ğŸ“š Exemples

### Exemple 1 : Rechercher "Avion" avec 8 rÃ©sultats

```bash
cargo run -- -k "Avion" -n 8
```

**Structure crÃ©Ã©e :**
```
resultats/
â””â”€â”€ Avion_20240116_143025/
    â”œâ”€â”€ RESUME_RECHERCHE.md    â† RÃ©sumÃ© global avec tableau et stats
    â”œâ”€â”€ Avion.md
    â”œâ”€â”€ Avion_de_ligne.md
    â””â”€â”€ ... (7 autres articles)
```

**Contenu du RESUME_RECHERCHE.md :**
- ğŸ“‹ Tableau rÃ©capitulatif de tous les articles scrapÃ©s
- ğŸ“– RÃ©sumÃ©s courts (300 caractÃ¨res) de chaque article
- ğŸ“Š Statistiques globales (total sections, liens, images, moyennes)
- ğŸ”— Liens vers chaque dossier d'article

### Exemple 2 : Mode interactif

```bash
cargo run

# > Choix : 2
# > Mot-clÃ© : BMW
# > Nombre : 10  (ou EntrÃ©e pour dÃ©faut 5)
```

### Exemple 3 : Binaire compilÃ© (plus rapide)

```bash
cargo build --release
./target/release/wikipedia_scraper -k "Rust" -n 3
```

## ğŸ“„ Structure des fichiers gÃ©nÃ©rÃ©s

### Par recherche (dossier parent)

```
resultats/Avion_20240116_143025/
â”œâ”€â”€ RESUME_RECHERCHE.md          # â† Nouveau ! RÃ©sumÃ© global
â”œâ”€â”€ Avion.md                       # Article 1
â”œâ”€â”€ Avion_de_ligne.md              # Article 2
â””â”€â”€ Boeing_747.md                  # Article 3
```

### Par article (sous-dossier)

```
Avion/
â”œâ”€â”€ data.json          # Toutes les donnÃ©es structurÃ©es en JSON
â”œâ”€â”€ article.md         # Article formatÃ© en Markdown
â”œâ”€â”€ resume.txt         # Titre, URL et rÃ©sumÃ©
â”œâ”€â”€ sections.txt       # Liste des sections (une par ligne)
â”œâ”€â”€ liens.txt          # URLs des liens internes (une par ligne)
â””â”€â”€ images.txt         # URLs des images (une par ligne)
```

## ğŸ”§ RÃ©solution des problÃ¨mes

### Erreur "AccÃ¨s refusÃ©" (Windows)

**Solution** : DÃ©placez le projet hors de OneDrive
```powershell
# Vers:
C:\Users\Admin\Documents\RUST\Scrappeur_wikipedia
```

### Erreur de compilation

```bash
cargo clean
cargo build
```

### Aucun rÃ©sultat trouvÃ©

- VÃ©rifiez l'orthographe du mot-clÃ©
- Essayez avec un mot-clÃ© plus gÃ©nÃ©ral
- VÃ©rifiez votre connexion Internet

## ğŸ“ Cas d'usage

- ğŸ“š Recherche documentaire automatisÃ©e
- ğŸ”¬ Collecte de donnÃ©es pour projets acadÃ©miques
- ğŸ¤– Construction de datasets pour ML/AI
- ğŸ“Š Analyse de contenu Wikipedia en masse

## ğŸ“ Respect de Wikipedia

Ce scrappeur :
- âœ… Ajoute des pauses entre les requÃªtes (1 seconde)
- âœ… Utilise un User-Agent appropriÃ©
- âœ… Utilise l'API OpenSearch officielle
- âœ… Ne surcharge pas les serveurs

**Utilisez-le de maniÃ¨re responsable et Ã©thique !**

## ğŸ¤ Contribution

Projet ESGI - BAC +4 RUST - 2024

## ğŸ“ Contact

Projet acadÃ©mique ESGI

---

**Note technique** : Ce projet implÃ©mente manuellement les requÃªtes HTTP tout en utilisant rustls uniquement pour la couche TLS/HTTPS (nÃ©cessaire car Wikipedia force HTTPS). Cela respecte l'esprit de la contrainte "pas de bibliothÃ¨que de haut niveau pour les requÃªtes rÃ©seau".
